\documentclass[a4paper, 12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[left=2cm,right=2cm,top=3cm,bottom=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{biblatex}
\usepackage{mathtools}
\usepackage{xcolor}

\newcommand{\defeq}{\stackrel{\text{def}}{=}}
\newcommand{\norm}[1]{\lVert #1 \rVert_2}
\newcommand{\re}[2]{\text{RE}(#1 \Vert #2)}
\newcommand{\inner}[2]{\langle #1, #2 \rangle}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator{\dom}{dom}

\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\addbibresource[]{ref.bib}

\title{CSIE5002 Prediction, learning, and games HW0}
\author{Hsiung, Yu-Ting B07902119}

\begin{document}
\maketitle

\begin{enumerate}
    \item (14 points) What is the research paper you choose? Specify the authors, the title, and where and when the paper was published or released (an arXiv paper is fine).
    \par {\color{blue} \fullcite{freund2003predicting}.}
    \item (14 points) Why do you choose the paper?
    \par {\color{blue} I see this paper in the references section of ``Game theory, on-line prediction and boosting'' \cite{freund1996game} and I think that giving generalization to uncountably infinite sets of hypotheses is cool, so I started looking into this paper. (Through I don't have enough time to dive into the technical details about uncountable cases when writing this homework.) Also, the author Yoav Freund is known for the famous AdaBoost algorithm, so this made me more interested in it.}
    \item (14 points) Does the paper impose any assumption on the sequence $(\omega_t)_{t \in \N}$? If yes, what are the assumptions? Why are the assumptions reasonable?
    \par {\color{blue} The paper applies the most popular simple assumption: the sequence $(\omega_t)_{t \in \N}$ is generated by independent random coin flips of a coin with some fixed bias $p$. It is reasonable to evaluate the bias of a coin quickly and minimize the regret, i.e. the number of times we make a wrong guess.}

    \item (14 points) Describe the algorithm for \textsc{Learner} you pick. Is the algorithm efficiently implementable?
    \par {\color{blue} Since \cite{freund2003predicting} use $\omega$ as a weight which associates to a model, I use $(x_t)_{t\in\N}$ here to denote the binary sequence we want to predict.
    \par Yoav denotes the algorithm by EW, standing for ``exponential weights''. First, denote a class of models by $\mathcal{P}$, and here I assume the finiteness that $\mathcal{P} = \{p_1, ..., p_N\} \subset [0, 1]$, i.e. $\mathcal{P}$ is finite. Denote the cumulative loss of the model $p_i$ at time $t$ by $L(p_i, t) = \sum_{t'=1}^t \ell(p_i, x_{t'})$, where $\ell$ is some loss function. Yoav considers the following three loss functions: the \textit{square loss} $\ell_2(p, x) = (x - p)^2$, the \textit{log loss} $\ell_{\log}(p, x) = -x \log p -(1-x) \log (1-p)$, the \textit{absolute loss} $\ell_1(p, x) = |x - p|$.
    \par The algorithm takes a positive real number $\eta$ as parameter. With each model in the class, at each time step $t$, it associates a \textit{weight} $\omega_{t,i}=\exp(-\eta L(p_i, t))$. The initial weights sum to 1 and when the set of models is finite, the initial weights are usually set to $\omega_{1, i}=1/N$ for all models $i=1, ..., N$.
    \par The algorithm is pretty simple, so I think it should be efficiently implementable.
    }
    \item (14 points) What are the theoretical guarantees for the algorithm? Make sure you clearly specify the performance measures and do not miss any assumption on the problem set-up.
    \par {\color{blue} The worst case regret of the algorithm EW is
    \[R_{\text{wc}}(EW, t) \leq \frac{1}{\eta}\ln N\]
    where $\displaystyle R_{\text{wc}}(A, T) := \max_{\mathbf{x}^T}\left(\sum_{t=1}^T \ell(p_t, x_t) - \min_{p\in[0,1]}\sum_{t=1}^T\ell(p, x_t)\right)$ for an algorithm $A$.
    }
    \item (14 points) The game above is an abstraction of many real-world problems. Describe a real-world application
    to which you think both the algorithm and its theoretical guarantee are applicable and explain why. The realworld application can be a sub-routine in an algorithm.
    \par {\color{blue} The assumption is very simple and clear: a sequence generated by i.i.d Bernoulli random variables. The real-world application I first came up with is a two-way branch predictor. Let's say at time $t$, $\omega_t = 1$ if the conditional jump is taken, and $\omega_t=0$ if not taken. Here I assume the instructions are independent and the probability taking the conditional jump is fixed over all $t$.}

    \item (14 points) Describe a real-world binary sequential prediction problem for which you think the algorithm
    or its theoretical guarantee is not satisfactory and explain why. If you do not find such a real-world problem, then explain why you think the algorithm and its theoretical guarantee are satisfactory for every binary
    sequential prediction game.
    \par {\color{blue} The assumption of this problem is basically based on a biased coin flip, so any sequence that is not generated by i.i.d do not satisfy the problem. Therefore, sequences such like ``will the stock price of TSMC rises tomorrow?'' or ``is Michael Jackson dead today?'' do not satisfy the i.i.d assumption well.}
    \item (*) Do you notice any significant mistake in the proofs in the paper that is not immediately solvable? If yes,
    specify the mistake and explain why it is significant and cannot be immediately solved.
    \par {\color{blue} Not yet ;)}
\end{enumerate}

\printbibliography

\end{document}
